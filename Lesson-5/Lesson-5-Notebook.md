# ğŸ™Œ**ä¹¦ç”ŸÂ·æµ¦è¯­(InternLM)-openLesson-5**ğŸ™Œ
> **â€œäº‹æƒ…å¾ˆç®€å•ï¼Œæ‰€æœ‰çš„ç§˜å¯†åªæœ‰ä¸¤å¥è¯:ä¸å±ˆä¸æŒ ï¼ŒåšæŒåˆ°åº•ã€‚â€ -> é™€æ€å¦¥è€¶å¤«æ–¯åŸº**
## **LMDeploy å¤§æ¨¡å‹é‡åŒ–éƒ¨ç½²å®è·µ**
### **Introduction**
![Alt text](Pic/Bg-Pic-1.png)

**å¤§æ¨¡å‹éƒ¨ç½²å…·æœ‰ç‹¬ç‰¹çš„èƒŒæ™¯ï¼ŒæŒ‰ç…§æ¨¡å‹éƒ¨ç½²çš„æœ´å®å®šä¹‰ï¼Œæ˜¯å°†è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨ç‰¹å®šè½¯ç¡¬ä»¶ç¯å¢ƒä¸­å¯åŠ¨çš„è¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿæ¥æ”¶è¾“å…¥å¹¶è¿”å›é¢„æµ‹ç»“æœã€‚ä¸ºäº†æ»¡è¶³æ€§èƒ½å’Œæ•ˆç‡çš„è¦æ±‚ï¼Œå¸¸å¸¸éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–(æ¨¡å‹å‹ç¼©å’Œæ–‡ä»¶åŠ é€Ÿ)**
+ **å†…å­˜å¼€é”€å·¨å¤§**
+ **è¯·æ±‚æ•°ä¸å›ºå®š(åŠ¨æ€Shape)**
+ **LLMæ¨¡å‹çš„ç»“æ„ç›¸è¾ƒäºè§†è§‰æ¨¡å‹è€Œè¨€ï¼Œæ¯”è¾ƒç®€å•**

![Alt text](Pic/Bg-Pic-2.png)

### **å¦‚ä½•åš Weight Only çš„é‡åŒ–ï¼Ÿ**
![Alt text](Pic/Bg-Pic-3.png)

### **æ¨ç†å¼•æ“**
![Alt text](Pic/Bg-Pic-4.png)

![Alt text](Pic/Bg-Pic-5.png)

**æ¨ç†å¼•æ“ TurboMind ä¸»è¦åŒ…å«äº†ï¼š**
+ **æŒç»­æ‰¹å¤„ç†(å¾ˆæœ‰æ„æ€)**
+ **æœ‰çŠ¶æ€çš„æ¨ç†** 
+ **é«˜æ€§èƒ½ cuda kernel**
+ **Blocked k/v cache**

### **ä½œä¸šä»»åŠ¡ Demo ç›®æ ‡**
**åŸºç¡€ä½œä¸šï¼š**

+ **ä½¿ç”¨ LMDeploy ä»¥æœ¬åœ°å¯¹è¯ã€ç½‘é¡µGradioã€APIæœåŠ¡ä¸­çš„ä¸€ç§æ–¹å¼éƒ¨ç½² InternLM-Chat-7B æ¨¡å‹ï¼Œç”Ÿæˆ 300 å­—çš„å°æ•…äº‹ï¼ˆéœ€æˆªå›¾ï¼‰**

**è¿›é˜¶ä½œä¸šï¼ˆå¯é€‰åšï¼‰**

+ **å°†ç¬¬å››èŠ‚è¯¾è®­ç»ƒè‡ªæˆ‘è®¤çŸ¥å°åŠ©æ‰‹æ¨¡å‹ä½¿ç”¨ LMDeploy é‡åŒ–éƒ¨ç½²åˆ° OpenXLab å¹³å°ã€‚**

+ **å¯¹internlm-chat-7bæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œå¹¶åŒæ—¶ä½¿ç”¨KV Cacheé‡åŒ–ï¼Œä½¿ç”¨é‡åŒ–åçš„æ¨¡å‹å®ŒæˆAPIæœåŠ¡çš„éƒ¨ç½²ï¼Œåˆ†åˆ«å¯¹æ¯”æ¨¡å‹é‡åŒ–å‰åå’Œ KV Cache é‡åŒ–å‰åçš„æ˜¾å­˜å¤§å°ï¼ˆå°† bsè®¾ç½®ä¸º 1 å’Œ max len è®¾ç½®ä¸º512ï¼‰ã€‚åœ¨è‡ªå·±çš„ä»»åŠ¡æ•°æ®é›†ä¸Šä»»å–è‹¥å¹²æ¡è¿›è¡ŒBenchmarkæµ‹è¯•ï¼Œæµ‹è¯•æ–¹å‘åŒ…æ‹¬ï¼š**
    1. **TurboMindæ¨ç† + Pythonä»£ç é›†æˆ**
    2. **åœ¨ 1) çš„åŸºç¡€ä¸Šé‡‡ç”¨W4A16é‡åŒ–**
    3. **åœ¨ 1) çš„åŸºç¡€ä¸Šå¼€å¯KV Cacheé‡åŒ–**
    4. **åœ¨ 2) çš„åŸºç¡€ä¸Šå¼€å¯KV Cacheé‡åŒ–**
    5. **ä½¿ç”¨Huggingfaceæ¨ç†**

#### **åŸºç¡€ä½œä¸š(è¯¾ç¨‹è¦æ±‚é¡¹)**

##### **(1) ç¯å¢ƒé…ç½®**
**é¦–å…ˆæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ vgpu-smi  æŸ¥çœ‹æ˜¾å¡èµ„æºä½¿ç”¨æƒ…å†µã€‚**

**ä¹Ÿå¯ä»¥ç‚¹å‡»ç»ˆç«¯ï¼ˆTERMINALï¼‰çª—å£å³ä¾§çš„ã€Œ+ã€å·åˆ›å»ºæ–°çš„ç»ˆç«¯çª—å£ã€‚å¤§å®¶å¯ä»¥æ–°å¼€ä¸€ä¸ªçª—å£ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤å®æ—¶è§‚å¯Ÿ GPU èµ„æºçš„ä½¿ç”¨æƒ…å†µã€‚**

    $ watch vgpu-smi

**è¿™é‡Œ /share/conda_envs ç›®å½•ä¸‹çš„ç¯å¢ƒæ˜¯å®˜æ–¹å‡†å¤‡å¥½çš„åŸºç¡€ç¯å¢ƒï¼Œå› ä¸ºè¯¥ç›®å½•æ˜¯å…±äº«åªè¯»çš„ï¼Œè€Œæˆ‘ä»¬åé¢éœ€è¦åœ¨æ­¤åŸºç¡€ä¸Šå®‰è£…æ–°çš„è½¯ä»¶åŒ…ï¼Œæ‰€ä»¥éœ€è¦å¤åˆ¶åˆ°æˆ‘ä»¬è‡ªå·±çš„ conda ç¯å¢ƒã€‚**

    $ conda create -n CONDA_ENV_NAME --clone /share/conda_envs/internlm-base

**å¦‚æœcloneæ“ä½œè¿‡æ…¢ï¼Œå¯é‡‡ç”¨å¦‚ä¸‹æ“ä½œ:**

    $ /root/share/install_conda_env_internlm_base.sh lmdeploy

**æˆ‘ä»¬å– CONDA_ENV_NAME ä¸º lmdeployï¼Œå¤åˆ¶å®Œæˆåï¼Œå¯ä»¥åœ¨æœ¬åœ°æŸ¥çœ‹ç¯å¢ƒã€‚**

    $ conda env list

**ç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚**

    # conda environments:
    #
    base                  *  /root/.conda
    lmdeploy                 /root/.conda/envs/lmdeploy

**ç„¶åæ¿€æ´»ç¯å¢ƒã€‚**

    $ conda activate lmdeploy

**å®‰è£… LMDeploy**

    # è§£å†³ ModuleNotFoundError: No module named 'packaging' é—®é¢˜
    pip install packaging

    # ä½¿ç”¨ flash_attn çš„é¢„ç¼–è¯‘åŒ…è§£å†³å®‰è£…è¿‡æ…¢é—®é¢˜
    pip install /root/share/wheels/flash_attn-2.4.2+cu118torch2.0cxx11abiTRUE-cp310-cp310-linux_x86_64.whl

    pip install 'lmdeploy[all]==v0.1.0'

##### **(2) æœåŠ¡éƒ¨ç½²**
![Alt text](Pic/Bg-Pic-6.png)

**æˆ‘ä»¬æŠŠä»æ¶æ„ä¸ŠæŠŠæ•´ä¸ªæœåŠ¡æµç¨‹åˆ†æˆä¸‹é¢å‡ ä¸ªæ¨¡å—ã€‚**

**æ¨¡å‹æ¨ç†/æœåŠ¡ã€‚ä¸»è¦æä¾›æ¨¡å‹æœ¬èº«çš„æ¨ç†ï¼Œä¸€èˆ¬æ¥è¯´å¯ä»¥å’Œå…·ä½“ä¸šåŠ¡è§£è€¦ï¼Œä¸“æ³¨æ¨¡å‹æ¨ç†æœ¬èº«æ€§èƒ½çš„ä¼˜åŒ–ã€‚å¯ä»¥ä»¥æ¨¡å—ã€APIç­‰å¤šç§æ–¹å¼æä¾›ã€‚**
+ **Client - å¯ä»¥ç†è§£ä¸ºå‰ç«¯ï¼Œä¸ç”¨æˆ·äº¤äº’çš„åœ°æ–¹ã€‚**
+ **API Server - ä¸€èˆ¬ä½œä¸ºå‰ç«¯çš„åç«¯ï¼Œæä¾›ä¸äº§å“å’ŒæœåŠ¡ç›¸å…³çš„æ•°æ®å’ŒåŠŸèƒ½æ”¯æŒã€‚**

**lmdeploy æ”¯æŒç›´æ¥è¯»å– Huggingface æ¨¡å‹æƒé‡**

    # éœ€è¦èƒ½è®¿é—® Huggingface çš„ç½‘ç»œç¯å¢ƒ
    lmdeploy chat turbomind internlm/internlm-chat-20b-4bit --model-name internlm-chat-20b
    lmdeploy chat turbomind Qwen/Qwen-7B-Chat --model-name qwen-7b

**æˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥å¯åŠ¨æœ¬åœ°çš„ Huggingface æ¨¡å‹ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚**

    lmdeploy chat turbomind /share/temp/model_repos/internlm-chat-7b/  --model-name internlm-chat-7b

![Alt text](Pic/Bg-Pic-7.png)

##### **(3) Demo æ¼”ç¤º + TurboMind æ¨ç†(åŸºç¡€ä½œä¸š)**

**æ”¯æŒå¤šç§æ–¹å¼è¿è¡Œï¼Œæ¯”å¦‚Turbomindã€PyTorchã€DeepSpeedã€‚ä½† PyTorch å’Œ DeepSpeed è°ƒç”¨çš„å…¶å®éƒ½æ˜¯ Huggingface çš„ Transformers åŒ…ï¼ŒPyTorchè¡¨ç¤ºåŸç”Ÿçš„ Transformer åŒ…ï¼ŒDeepSpeed è¡¨ç¤ºä½¿ç”¨äº† DeepSpeed ä½œä¸ºæ¨ç†æ¡†æ¶ã€‚Pytorch/DeepSpeed ç›®å‰åŠŸèƒ½éƒ½æ¯”è¾ƒå¼±ï¼Œä¸å…·å¤‡ç”Ÿäº§èƒ½åŠ›ï¼Œä¸æ¨èä½¿ç”¨ã€‚**

![Alt text](Pic/Bg-Pic-8.png)

**æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š**

    # Turbomind
    lmdeploy chat turbomind ./workspace

**æˆ–å¯åŠ¨å‘½ä»¤æœåŠ¡ï¼š**

    # ApiServer+Turbomind   api_server => AsyncEngine => TurboMind
    lmdeploy serve api_server ./workspace \
        --server_name 0.0.0.0 \
        --server_port 23333 \
        --instance_num 64 \
        --tp 1

**è‹¥ç»“åˆ gradio åˆ™ä¿®æ”¹å‘½ä»¤(ä¸ä¿®æ”¹ç»“æ„)ï¼š**

    # Gradio+ApiServerã€‚å¿…é¡»å…ˆå¼€å¯ Serverï¼Œæ­¤æ—¶ Gradio ä¸º Client
    lmdeploy serve gradio http://0.0.0.0:23333 \
        --server_name 0.0.0.0 \
        --server_port 6006 \
        --restful_api True

![Alt text](Pic/Bg-Pic-9.png)

---

![Alt text](Pic/Bg-Pic-10.png)

**å¯¹äºå…¶ç½‘ç»œé€»è¾‘ï¼Œæˆ‘ä»¬ç”¨ä»¥ä¸‹æ–¹å¼å®ç°ï¼š**

    from lmdeploy import turbomind as tm

    # load model
    model_path = "/root/share/temp/model_repos/internlm-chat-7b/"
    tm_model = tm.TurboMind.from_pretrained(model_path, model_name='internlm-chat-20b')
    generator = tm_model.create_instance()

    # process query
    query = "ä½ å¥½å•Šå…„å˜š"
    prompt = tm_model.model.get_prompt(query)
    input_ids = tm_model.tokenizer.encode(prompt)

    # inference
    for outputs in generator.stream_infer(
            session_id=0,
            input_ids=[input_ids]):
        res, tokens = outputs[0]

    response = tm_model.tokenizer.decode(res.tolist())
    print(response)

## è¡¥å……(è¿›é˜¶ä½œä¸šç›¸å…³)

### æ¨¡å‹é‡åŒ–(è¿›é˜¶ä½œä¸š Demo)

**KV Cache é‡åŒ–æ˜¯å°†å·²ç»ç”Ÿæˆåºåˆ—çš„ KV å˜æˆ Int8ï¼Œä½¿ç”¨è¿‡ç¨‹ä¸€å…±åŒ…æ‹¬ä¸‰æ­¥ï¼š**

**ç¬¬ä¸€æ­¥ï¼šè®¡ç®— minmaxã€‚ä¸»è¦æ€è·¯æ˜¯é€šè¿‡è®¡ç®—ç»™å®šè¾“å…¥æ ·æœ¬åœ¨æ¯ä¸€å±‚ä¸åŒä½ç½®å¤„è®¡ç®—ç»“æœçš„ç»Ÿè®¡æƒ…å†µã€‚**

**å¯¹äº Attention çš„ K å’Œ Vï¼šå–æ¯ä¸ª Head å„è‡ªç»´åº¦åœ¨æ‰€æœ‰Tokençš„æœ€å¤§ã€æœ€å°å’Œç»å¯¹å€¼æœ€å¤§å€¼ã€‚å¯¹æ¯ä¸€å±‚æ¥è¯´ï¼Œä¸Šé¢ä¸‰ç»„å€¼éƒ½æ˜¯ (num_heads, head_dim) çš„çŸ©é˜µã€‚è¿™é‡Œçš„ç»Ÿè®¡ç»“æœå°†ç”¨äºæœ¬å°èŠ‚çš„ KV Cacheã€‚å¯¹äºæ¨¡å‹æ¯å±‚çš„è¾“å…¥ï¼šå–å¯¹åº”ç»´åº¦çš„æœ€å¤§ã€æœ€å°ã€å‡å€¼ã€ç»å¯¹å€¼æœ€å¤§å’Œç»å¯¹å€¼å‡å€¼ã€‚æ¯ä¸€å±‚æ¯ä¸ªä½ç½®çš„è¾“å…¥éƒ½æœ‰å¯¹åº”çš„ç»Ÿè®¡å€¼ï¼Œå®ƒä»¬å¤§å¤šæ˜¯ (hidden_dim, ) çš„ä¸€ç»´å‘é‡ï¼Œå½“ç„¶åœ¨ FFN å±‚ç”±äºç»“æ„æ˜¯å…ˆå˜å®½åæ¢å¤ï¼Œå› æ­¤æ¢å¤çš„ä½ç½®ç»´åº¦å¹¶ä¸ç›¸åŒã€‚è¿™é‡Œçš„ç»Ÿè®¡ç»“æœç”¨äºä¸‹ä¸ªå°èŠ‚çš„æ¨¡å‹å‚æ•°é‡åŒ–ï¼Œä¸»è¦ç”¨åœ¨ç¼©æ”¾ç¯èŠ‚ã€‚**

**ç¬¬ä¸€æ­¥æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š**

    # è®¡ç®— minmax
    lmdeploy lite calibrate \
    --model  /root/share/temp/model_repos/internlm-chat-7b/ \
    --calib_dataset "c4" \
    --calib_samples 128 \
    --calib_seqlen 2048 \
    --work_dir ./quant_output

**åœ¨è¿™ä¸ªå‘½ä»¤è¡Œä¸­ï¼Œä¼šé€‰æ‹© 128 æ¡è¾“å…¥æ ·æœ¬ï¼Œæ¯æ¡æ ·æœ¬é•¿åº¦ä¸º 2048ï¼Œæ•°æ®é›†é€‰æ‹© C4ï¼Œè¾“å…¥æ¨¡å‹åå°±ä¼šå¾—åˆ°ä¸Šé¢çš„å„ç§ç»Ÿè®¡å€¼ã€‚å€¼å¾—è¯´æ˜çš„æ˜¯ï¼Œå¦‚æœæ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥é€‚å½“è°ƒå° samples çš„æ•°é‡æˆ– sample çš„é•¿åº¦ã€‚**

**è¿™ä¸€æ­¥ç”±äºé»˜è®¤éœ€è¦ä» Huggingface ä¸‹è½½æ•°æ®é›†ï¼Œå›½å†…ç»å¸¸ä¸æˆåŠŸã€‚æ‰€ä»¥æˆ‘ä»¬å¯¼å‡ºäº†éœ€è¦çš„æ•°æ®ï¼Œå¤§å®¶éœ€è¦å¯¹è¯»å–æ•°æ®é›†çš„ä»£ç æ–‡ä»¶åšä¸€ä¸‹æ›¿æ¢ã€‚å…±åŒ…æ‹¬ä¸¤æ­¥ï¼š**

1. **å¤åˆ¶ calib_dataloader.py åˆ°å®‰è£…ç›®å½•æ›¿æ¢è¯¥æ–‡ä»¶ï¼šcp /root/share/temp/datasets/c4/calib_dataloader.py  /root/.conda/envs/lmdeploy/lib/python3.10/site-packages/lmdeploy/lite/utils/**
2. **å°†ç”¨åˆ°çš„æ•°æ®é›†ï¼ˆc4ï¼‰å¤åˆ¶åˆ°ä¸‹é¢çš„ç›®å½•ï¼šcp -r /root/share/temp/datasets/c4/ /root/.cache/huggingface/datasets/**


**ç¬¬äºŒæ­¥ï¼šé€šè¿‡ minmax è·å–é‡åŒ–å‚æ•°ã€‚ä¸»è¦å°±æ˜¯åˆ©ç”¨ä¸‹é¢è¿™ä¸ªå…¬å¼ï¼Œè·å–æ¯ä¸€å±‚çš„ K V ä¸­å¿ƒå€¼(zp)å’Œç¼©æ”¾å€¼(scale)ã€‚**

    zp = (min+max) / 2
    scale = (max-min) / 255
    quant: q = round( (f-zp) / scale)
    dequant: f = q * scale + zp

**æœ‰è¿™ä¸¤ä¸ªå€¼å°±å¯ä»¥è¿›è¡Œé‡åŒ–å’Œè§£é‡åŒ–æ“ä½œäº†ã€‚å…·ä½“æ¥è¯´ï¼Œå°±æ˜¯å¯¹å†å²çš„ K å’Œ V å­˜å‚¨ quant åçš„å€¼ï¼Œä½¿ç”¨æ—¶åœ¨ dequantã€‚**

![Alt text](Pic/Bg-Pic-11.png)

**ç¬¬äºŒæ­¥çš„æ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š**

    # é€šè¿‡ minmax è·å–é‡åŒ–å‚æ•°
    lmdeploy lite kv_qparams \
    --work_dir ./quant_output  \
    --turbomind_dir workspace/triton_models/weights/ \
    --kv_sym False \
    --num_tp 1

**åœ¨è¿™ä¸ªå‘½ä»¤ä¸­ï¼Œnum_tp çš„å«ä¹‰å‰é¢ä»‹ç»è¿‡ï¼Œè¡¨ç¤º Tensor çš„å¹¶è¡Œæ•°ã€‚æ¯ä¸€å±‚çš„ä¸­å¿ƒå€¼å’Œç¼©æ”¾å€¼ä¼šå­˜å‚¨åˆ° workspace çš„å‚æ•°ç›®å½•ä¸­ä»¥ä¾¿åç»­ä½¿ç”¨ã€‚kv_sym ä¸º True æ—¶ä¼šä½¿ç”¨å¦ä¸€ç§ï¼ˆå¯¹ç§°ï¼‰é‡åŒ–æ–¹æ³•ï¼Œå®ƒç”¨åˆ°äº†ç¬¬ä¸€æ­¥å­˜å‚¨çš„ç»å¯¹å€¼æœ€å¤§å€¼ï¼Œè€Œä¸æ˜¯æœ€å¤§å€¼å’Œæœ€å°å€¼ã€‚**

**ç¬¬ä¸‰æ­¥ï¼šä¿®æ”¹é…ç½®ã€‚ä¹Ÿå°±æ˜¯ä¿®æ”¹ weights/config.ini æ–‡ä»¶çš„ KV int8 å¼€å…³ï¼Œåªéœ€è¦æŠŠ quant_policy æ”¹ä¸º 4 å³å¯ã€‚**

**è¿è¡Œæ•ˆæœå¦‚ä¸‹æ‰€ç¤º**

---

![Alt text](Pic/Bg-Pic-12.png)

---

**W4A16ä¸­çš„Aæ˜¯æŒ‡Activationï¼Œä¿æŒFP16ï¼Œåªå¯¹å‚æ•°è¿›è¡Œ 4bit é‡åŒ–ã€‚ä½¿ç”¨è¿‡ç¨‹ä¹Ÿå¯ä»¥çœ‹ä½œæ˜¯ä¸‰æ­¥ã€‚**

**ç¬¬ä¸€æ­¥ï¼šåŒ 1.3.1ï¼Œä¸å†èµ˜è¿°ã€‚**

**ç¬¬äºŒæ­¥ï¼šé‡åŒ–æƒé‡æ¨¡å‹ã€‚åˆ©ç”¨ç¬¬ä¸€æ­¥å¾—åˆ°çš„ç»Ÿè®¡å€¼å¯¹å‚æ•°è¿›è¡Œé‡åŒ–ï¼Œæ‰§è¡Œå‘½ä»¤å¦‚ä¸‹ï¼š**

    # é‡åŒ–æƒé‡æ¨¡å‹
    lmdeploy lite auto_awq \
    --model  /root/share/temp/model_repos/internlm-chat-7b/ \
    --w_bits 4 \
    --w_group_size 128 \
    --work_dir ./quant_output 

**å‘½ä»¤ä¸­ w_bits è¡¨ç¤ºé‡åŒ–çš„ä½æ•°ï¼Œw_group_size è¡¨ç¤ºé‡åŒ–åˆ†ç»„ç»Ÿè®¡çš„å°ºå¯¸ï¼Œwork_dir æ˜¯é‡åŒ–åæ¨¡å‹è¾“å‡ºçš„ä½ç½®ã€‚è¿™é‡Œéœ€è¦ç‰¹åˆ«è¯´æ˜çš„æ˜¯ï¼Œå› ä¸ºæ²¡æœ‰ torch.int4ï¼Œæ‰€ä»¥å®é™…å­˜å‚¨æ—¶ï¼Œ8ä¸ª 4bit æƒé‡ä¼šè¢«æ‰“åŒ…åˆ°ä¸€ä¸ª int32 å€¼ä¸­ã€‚æ‰€ä»¥ï¼Œå¦‚æœä½ æŠŠè¿™éƒ¨åˆ†é‡åŒ–åçš„å‚æ•°åŠ è½½è¿›æ¥å°±ä¼šå‘ç°å®ƒä»¬æ˜¯ int32 ç±»å‹çš„ã€‚**

![Alt text](Pic/Bg-Pic-13.png)

**æœ€åä¸€æ­¥ï¼šè½¬æ¢æˆ TurboMind æ ¼å¼ã€‚**

    # è½¬æ¢æ¨¡å‹çš„layoutï¼Œå­˜æ”¾åœ¨é»˜è®¤è·¯å¾„ ./workspace ä¸‹
    lmdeploy convert  internlm-chat-7b ./quant_output \
        --model-format awq \
        --group-size 128

**è¿™ä¸ª group-size å°±æ˜¯ä¸Šä¸€æ­¥çš„é‚£ä¸ª w_group_sizeã€‚å¦‚æœä¸æƒ³å’Œä¹‹å‰çš„ workspace é‡å¤ï¼Œå¯ä»¥æŒ‡å®šè¾“å‡ºç›®å½•ï¼š--dst_pathï¼Œæ¯”å¦‚ï¼š**

    lmdeploy convert  internlm-chat-7b ./quant_output \
        --model-format awq \
        --group-size 128 \
        --dst_path ./workspace_quant

**è¿è¡Œæ•ˆæœå¦‚ä¸‹æ‰€ç¤ºï¼Œå¯ä»¥å‘ç°æ˜¾å­˜å ç”¨å¤§å¹…åº¦é™ä½ã€‚****

![Alt text](Pic/Bg-Pic-14.png)

**è€ƒè™‘åˆ°æ¨¡å‹å¤§ä½œä¸šæœ‰å¾ˆå¤šæŠ€æœ¯åˆ†æ•°ï¼Œå…¶ä»–è¿›é˜¶ä½œä¸š(ä¸Šä¼ ä¸æµ‹è¯•)ä¼šåœ¨åç»­å¤§ä½œä¸šéƒ¨ç½²ä¹‹ä¸­å®Œæˆã€‚**

## é“¾æ¥
**è§†é¢‘ï¼šLMDeploy å¤§æ¨¡å‹é‡åŒ–éƒ¨ç½²å®è·µ**

https://www.bilibili.com/video/BV1iW4y1A77P/?spm_id_from=333.788&vd_source=cb911a92ddd7e0d930b1daa60c3fc181
