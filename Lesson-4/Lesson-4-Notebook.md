# ğŸ˜µâ€ğŸ’«**ä¹¦ç”ŸÂ·æµ¦è¯­(InternLM)-openLesson-4**ğŸ˜µâ€ğŸ’«
> **â€œæˆ‘ä¸å¿§ä¼¤ï¼Œä¹Ÿä¸æ³„æ°”ã€‚â€ -> é™€æ€å¦¥è€¶å¤«æ–¯åŸº**
## **XTuner å¤§æ¨¡å‹å•å¡ä½æˆæœ¬å¾®è°ƒå®æˆ˜**
### **è§£æå¯¹è¯æ¨¡æ¿ä¸æŒ‡ä»¤å¾®è°ƒ**
![Alt text](Pic/Bg-Pic-1.png)
**åœ¨å®é™…å¯¹è¯ä¸­ï¼Œé€šå¸¸æœ‰ä¸‰ç§è§’è‰²ï¼š**
+ **Systemï¼šç»™å®šä¸€äº›ä¸Šä¸‹æ–‡ä¿¡æ¯**
+ **Userï¼šå®é™…ç”¨æˆ·ï¼Œä¼šæå‡ºä¸€äº›é—®é¢˜**
+ **Assistantï¼šæ ¹æ®Userçš„è¾“å…¥ï¼Œç»“åˆSystemçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œåšå‡ºå›ç­”**
---

![Alt text](Pic/Bg-Pic-2.png)
**ä¸åŒäºå¢é‡é¢„è®­ç»ƒå¾®è°ƒï¼ŒæŒ‡ä»¤è·Ÿéšå¾®è°ƒæ•°æ®ä¸­ä¼šæœ‰ Input å’Œ Output å¸Œæœ›æ¨¡å‹å­¦ä¼šçš„æ˜¯ç­”æ¡ˆ(Output)è€Œä¸æ˜¯é—®é¢˜(Input)ï¼Œè®­ç»ƒæ—¶åªä¼šå¯¹ç­”æ¡ˆéƒ¨åˆ†è®¡ç®—Lossã€‚è®­ç»ƒæ—¶ï¼Œå›åˆæ¨ç†ä¿æŒä¸€ç›´ï¼Œå¯¹æ•°æ®æ·»åŠ ç›¸åº”çš„å¯¹è¯æ¨¡æ¿ã€‚**

---

![Alt text](Pic/Bg-Pic-3.png)
**å¢é‡æ•°æ®å¾®è°ƒæœ€ç»ˆè¦çš„ä¸åŒåœ¨äºï¼šâ€œè®©LLMçŸ¥é“ä»€ä¹ˆæ—¶å€™å¼€å§‹ä¸€æ®µè¯ï¼Œä»€ä¹ˆæ—¶å€™ç»“æŸä¸€æ®µè¯ã€‚â€**

### **LoRA & QLoRA**
![Alt text](Pic/Bg-Pic-4.png)

### **å¿«é€Ÿä¸Šæ‰‹**

    # å¦‚æœä½ æ˜¯åœ¨ InternStudio å¹³å°ï¼Œåˆ™ä»æœ¬åœ° clone ä¸€ä¸ªå·²æœ‰ pytorch 2.0.1 çš„ç¯å¢ƒï¼š
    /root/share/install_conda_env_internlm_base.sh xtuner0.1.9
    # å¦‚æœä½ æ˜¯åœ¨å…¶ä»–å¹³å°ï¼š
    conda create --name xtuner0.1.9 python=3.10 -y

    # æ¿€æ´»ç¯å¢ƒ
    conda activate xtuner0.1.9
    # è¿›å…¥å®¶ç›®å½• ï¼ˆ~çš„æ„æ€æ˜¯ â€œå½“å‰ç”¨æˆ·çš„homeè·¯å¾„â€ï¼‰
    cd ~
    # åˆ›å»ºç‰ˆæœ¬æ–‡ä»¶å¤¹å¹¶è¿›å…¥ï¼Œä»¥è·Ÿéšæœ¬æ•™ç¨‹
    mkdir xtuner019 && cd xtuner019


    # æ‹‰å– 0.1.9 çš„ç‰ˆæœ¬æºç 
    git clone -b v0.1.9  https://github.com/InternLM/xtuner
    # æ— æ³•è®¿é—®githubçš„ç”¨æˆ·è¯·ä» gitee æ‹‰å–:
    # git clone -b v0.1.9 https://gitee.com/Internlm/xtuner

    # è¿›å…¥æºç ç›®å½•
    cd xtuner

    # ä»æºç å®‰è£… XTuner
    pip install -e '.[all]'

**å‡†å¤‡åœ¨ oasst1 æ•°æ®é›†ä¸Šå¾®è°ƒ internlm-7b-chat**

    # åˆ›å»ºä¸€ä¸ªå¾®è°ƒ oasst1 æ•°æ®é›†çš„å·¥ä½œè·¯å¾„ï¼Œè¿›å…¥
    mkdir ~/ft-oasst1 && cd ~/ft-oasst1

### **é…ç½®ä¸å¾®è°ƒ**

**XTuner æä¾›å¤šä¸ªå¼€ç®±å³ç”¨çš„é…ç½®æ–‡ä»¶ï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ä¸‹åˆ—å‘½ä»¤æŸ¥çœ‹ï¼š**

    # åˆ—å‡ºæ‰€æœ‰å†…ç½®é…ç½®
    xtuner list-cfg

**æ‹·è´ä¸€ä¸ªé…ç½®æ–‡ä»¶åˆ°å½“å‰ç›®å½•ï¼š**

    # xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}

**åœ¨æœ¬æ¡ˆä¾‹ä¸­å³ï¼šï¼ˆæ³¨æ„æœ€åæœ‰ä¸ªè‹±æ–‡å¥å·ï¼Œä»£è¡¨å¤åˆ¶åˆ°å½“å‰è·¯å¾„ï¼‰**

    cd ~/ft-oasst1
    xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .

**é…ç½®æ–‡ä»¶åçš„è§£é‡Šï¼š**

    xtuner copy-cfg internlm_chat_7b_qlora_oasst1_e3 .


| æ¨¡å‹å | internlm_chat_7b |
|-------|-------|
| ä½¿ç”¨ç®—æ³• | qlora |
| æ•°æ®é›† | oasst1 |
| æŠŠæ•°æ®é›†è·‘å‡ æ¬¡| è·‘3æ¬¡ï¼še3 (epoch 3 ) |

æ—  chatæ¯”å¦‚ internlm-7b ä»£è¡¨æ˜¯åŸºåº§(base)æ¨¡å‹

### **æ¨¡å‹ä¸æ•°æ®é›†**

    # åˆ›å»ºä¸€ä¸ªç›®å½•ï¼Œæ”¾æ¨¡å‹æ–‡ä»¶ï¼Œé˜²æ­¢æ•£è½ä¸€åœ°
    mkdir ~/ft-oasst1/internlm-chat-7b

    # è£…ä¸€ä¸‹æ‹‰å–æ¨¡å‹æ–‡ä»¶è¦ç”¨çš„åº“
    pip install modelscope

    # ä» modelscope ä¸‹è½½ä¸‹è½½æ¨¡å‹æ–‡ä»¶
    cd ~/ft-oasst1
    apt install git git-lfs -y
    git lfs install
    git lfs clone https://modelscope.cn/Shanghai_AI_Laboratory/internlm-chat-7b.git -b v1.0.3

https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/main

**ç”±äº huggingface ç½‘ç»œé—®é¢˜ï¼Œå°†ä»¥ä¸‹æŒ‡ä»¤å¤åˆ¶åˆ°æ­£ç¡®ä½ç½®å³å¯(æ•™å­¦å¹³å°)ï¼š**

    cd ~/ft-oasst1
    # ...-guanaco åé¢æœ‰ä¸ªç©ºæ ¼å’Œè‹±æ–‡å¥å·å•Š
    cp -r /root/share/temp/datasets/openassistant-guanaco .

### **ä¿®æ”¹é…ç½®æ–‡ä»¶**

**ä¿®æ”¹å…¶ä¸­çš„æ¨¡å‹å’Œæ•°æ®é›†ä¸º æœ¬åœ°è·¯å¾„**

    cd ~/ft-oasst1
    vim internlm_chat_7b_qlora_oasst1_e3_copy.py

**åœ¨vimç•Œé¢å®Œæˆä¿®æ”¹åï¼Œè¯·è¾“å…¥:wqé€€å‡ºã€‚å‡å¦‚è®¤ä¸ºæ”¹é”™äº†å¯ä»¥ç”¨:q!é€€å‡ºä¸”ä¸ä¿å­˜ã€‚å½“ç„¶æˆ‘ä»¬ä¹Ÿå¯ä»¥è€ƒè™‘æ‰“å¼€pythonæ–‡ä»¶ç›´æ¥ä¿®æ”¹ï¼Œä½†æ³¨æ„ä¿®æ”¹å®Œåéœ€è¦æŒ‰ä¸‹Ctrl+Sè¿›è¡Œä¿å­˜ã€‚å‡å·ä»£è¡¨è¦åˆ é™¤çš„è¡Œï¼ŒåŠ å·ä»£è¡¨è¦å¢åŠ çš„è¡Œã€‚**

    # ä¿®æ”¹æ¨¡å‹ä¸ºæœ¬åœ°è·¯å¾„
    - pretrained_model_name_or_path = 'internlm/internlm-chat-7b'
    + pretrained_model_name_or_path = './internlm-chat-7b'

    # ä¿®æ”¹è®­ç»ƒæ•°æ®é›†ä¸ºæœ¬åœ°è·¯å¾„
    - data_path = 'timdettmers/openassistant-guanaco'
    + data_path = './openassistant-guanaco'

### **å¼€å§‹å¾®è°ƒ**

    # å•å¡
    ## ç”¨åˆšæ‰æ”¹å¥½çš„configæ–‡ä»¶è®­ç»ƒ
    xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

    # å¤šå¡
    NPROC_PER_NODE=${GPU_NUM} xtuner train ./internlm_chat_7b_qlora_oasst1_e3_copy.py

    # è‹¥è¦å¼€å¯ deepspeed åŠ é€Ÿï¼Œå¢åŠ  --deepspeed deepspeed_zero2 å³å¯

**å°†å¾—åˆ°çš„ PTH æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ¨¡å‹ï¼Œå³ï¼šç”Ÿæˆ Adapter æ–‡ä»¶å¤¹ï¼Œæ ¼å¼ä¸ºï¼š**

    xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH_file_dir} ${SAVE_PATH}

**å…·ä½“å‘½ä»¤ä¸º(æ•™è‚²å¹³å°)ï¼š**

    mkdir hf
    export MKL_SERVICE_FORCE_INTEL=1

    xtuner convert pth_to_hf ./internlm_chat_7b_qlora_oasst1_e3_copy.py ./work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_1.pth ./hf

### **éƒ¨ç½²ä¸æµ‹è¯•**
**å°† HuggingFace adapter åˆå¹¶åˆ°å¤§è¯­è¨€æ¨¡å‹ï¼š**

    xtuner convert merge ./internlm-chat-7b ./hf ./merged --max-shard-size 2GB
    # xtuner convert merge \
    #     ${NAME_OR_PATH_TO_LLM} \
    #     ${NAME_OR_PATH_TO_ADAPTER} \
    #     ${SAVE_PATH} \
    #     --max-shard-size 2GB

**ä¸åˆå¹¶åçš„æ¨¡å‹å¯¹è¯ï¼š**

    # åŠ è½½ Adapter æ¨¡å‹å¯¹è¯ï¼ˆFloat 16ï¼‰
    xtuner chat ./merged --prompt-template internlm_chat

    # 4 bit é‡åŒ–åŠ è½½
    # xtuner chat ./merged --bits 4 --prompt-template internlm_chat

**ä¿®æ”¹ cli_demo.py ä¸­çš„æ¨¡å‹è·¯å¾„**

    - model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"
    + model_name_or_path = "merged"

**è¿è¡Œ cli_demo.py ä»¥ç›®æµ‹å¾®è°ƒæ•ˆæœ**

    python ./cli_demo.py

## ä½œä¸šä»»åŠ¡ Demo

![Alt text](Pic/Bg-Pic-5.png)

+ åŸºç¡€ä½œä¸šï¼šæ„å»ºæ•°æ®é›†ï¼Œä½¿ç”¨ XTuner å¾®è°ƒ InternLM-Chat-7B æ¨¡å‹, è®©æ¨¡å‹å­¦ä¹ åˆ°å®ƒæ˜¯ä½ çš„æ™ºèƒ½å°åŠ©æ‰‹ï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œæœ¬ä½œä¸šè®­ç»ƒå‡ºæ¥çš„æ¨¡å‹çš„è¾“å‡ºéœ€è¦å°†ä¸è¦è‘±å§œè’œå¤§ä½¬æ›¿æ¢æˆè‡ªå·±åå­—æˆ–æ˜µç§°ï¼
+ è¿›é˜¶ä½œä¸šï¼šå°†è®­ç»ƒå¥½çš„Adapteræ¨¡å‹æƒé‡ä¸Šä¼ åˆ° OpenXLabã€Hugging Face æˆ–è€… MoelScope ä»»ä¸€ä¸€å¹³å°ã€‚å°†è®­ç»ƒå¥½åçš„æ¨¡å‹åº”ç”¨éƒ¨ç½²åˆ° OpenXLab å¹³å°ï¼Œå‚è€ƒéƒ¨ç½²æ–‡æ¡£è¯·è®¿é—®ï¼šhttps://aicarrier.feishu.cn/docx/MQH6dygcKolG37x0ekcc4oZhnCe

### **æ•°æ®å‡†å¤‡**

**åˆ›å»ºdataæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾ç”¨äºè®­ç»ƒçš„æ•°æ®é›†**

    mkdir -p /root/xtuner019/data && cd /root/xtuner019/data

**åœ¨dataç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªjsonæ–‡ä»¶personal_assistant.jsonä½œä¸ºæœ¬æ¬¡å¾®è°ƒæ‰€ä½¿ç”¨çš„æ•°æ®é›†ã€‚jsonä¸­å†…å®¹å¯å‚è€ƒä¸‹æ–¹(å¤åˆ¶ç²˜è´´næ¬¡åšæ•°æ®å¢å¹¿ï¼Œæ•°æ®é‡å°æ— æ³•æœ‰æ•ˆå¾®è°ƒï¼Œä¸‹é¢ä»…ç”¨äºå±•ç¤ºæ ¼å¼ï¼Œä¸‹é¢ä¹Ÿæœ‰ç”Ÿæˆè„šæœ¬)**

**å…¶ä¸­conversationè¡¨ç¤ºä¸€æ¬¡å¯¹è¯çš„å†…å®¹ï¼Œinputä¸ºè¾“å…¥ï¼Œå³ç”¨æˆ·ä¼šé—®çš„é—®é¢˜ï¼Œoutputä¸ºè¾“å‡ºï¼Œå³æƒ³è¦æ¨¡å‹å›ç­”çš„ç­”æ¡ˆã€‚**

    [
        {
            "conversation": [
                {
                    "input": "è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
                    "output": "æˆ‘æ˜¯é‚£è·¯çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦"
                }
            ]
        },
        {
            "conversation": [
                {
                    "input": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»",
                    "output": "æˆ‘æ˜¯é‚£è·¯çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦"
                }
            ]
        }
    ]

**ä»¥ä¸‹æ˜¯ä¸€ä¸ªpythonè„šæœ¬ï¼Œç”¨äºç”Ÿæˆæ•°æ®é›†ã€‚åœ¨dataç›®å½•ä¸‹æ–°å»ºä¸€ä¸ªgenerate_data.pyæ–‡ä»¶ï¼Œå°†ä»¥ä¸‹ä»£ç å¤åˆ¶è¿›å»ï¼Œç„¶åè¿è¡Œè¯¥è„šæœ¬å³å¯ç”Ÿæˆæ•°æ®é›†ã€‚**

    import json

    # è¾“å…¥ä½ çš„åå­—
    name = 'Shengshenlan'
    # é‡å¤æ¬¡æ•°
    n = 10000

    data = [
        {
            "conversation": [
                {
                    "input": "è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»",
                    "output": "æˆ‘æ˜¯{}çš„å°åŠ©æ‰‹ï¼Œå†…åœ¨æ˜¯ä¸Šæµ·AIå®éªŒå®¤ä¹¦ç”ŸÂ·æµ¦è¯­çš„7Bå¤§æ¨¡å‹å“¦".format(name)
                }
            ]
        }
    ]

    for i in range(n):
        data.append(data[0])

    with open('personal_assistant.json', 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=4)

### **é…ç½®å‡†å¤‡**

**é€šè¿‡ä»¥ä¸‹ä»£ç è¿›è¡Œé…ç½®ï¼š**

    # Copyright (c) OpenMMLab. All rights reserved.
    import torch
    from bitsandbytes.optim import PagedAdamW32bit
    from datasets import load_dataset
    from mmengine.dataset import DefaultSampler
    from mmengine.hooks import (CheckpointHook, DistSamplerSeedHook, IterTimerHook,
                                LoggerHook, ParamSchedulerHook)
    from mmengine.optim import AmpOptimWrapper, CosineAnnealingLR
    from peft import LoraConfig
    from transformers import (AutoModelForCausalLM, AutoTokenizer,
                            BitsAndBytesConfig)

    from xtuner.dataset import process_hf_dataset
    from xtuner.dataset.collate_fns import default_collate_fn
    from xtuner.dataset.map_fns import oasst1_map_fn, template_map_fn_factory
    from xtuner.engine import DatasetInfoHook, EvaluateChatHook
    from xtuner.model import SupervisedFinetune
    from xtuner.utils import PROMPT_TEMPLATE

    #######################################################################
    #                          PART 1  Settings                           #
    #######################################################################
    # Model
    pretrained_model_name_or_path = '/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b'

    # Data
    data_path = '/root/personal_assistant/data/personal_assistant.json'
    prompt_template = PROMPT_TEMPLATE.internlm_chat
    max_length = 512
    pack_to_max_length = True

    # Scheduler & Optimizer
    batch_size = 2  # per_device
    accumulative_counts = 16
    dataloader_num_workers = 0
    max_epochs = 3
    optim_type = PagedAdamW32bit
    lr = 2e-4
    betas = (0.9, 0.999)
    weight_decay = 0
    max_norm = 1  # grad clip

    # Evaluate the generation performance during the training
    evaluation_freq = 90
    SYSTEM = ''
    evaluation_inputs = [ 'è¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±', 'è¯·åšä¸€ä¸‹è‡ªæˆ‘ä»‹ç»' ]

    #######################################################################
    #                      PART 2  Model & Tokenizer                      #
    #######################################################################
    tokenizer = dict(
        type=AutoTokenizer.from_pretrained,
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        trust_remote_code=True,
        padding_side='right')

    model = dict(
        type=SupervisedFinetune,
        llm=dict(
            type=AutoModelForCausalLM.from_pretrained,
            pretrained_model_name_or_path=pretrained_model_name_or_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            quantization_config=dict(
                type=BitsAndBytesConfig,
                load_in_4bit=True,
                load_in_8bit=False,
                llm_int8_threshold=6.0,
                llm_int8_has_fp16_weight=False,
                bnb_4bit_compute_dtype=torch.float16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type='nf4')),
        lora=dict(
            type=LoraConfig,
            r=64,
            lora_alpha=16,
            lora_dropout=0.1,
            bias='none',
            task_type='CAUSAL_LM'))

    #######################################################################
    #                      PART 3  Dataset & Dataloader                   #
    #######################################################################
    train_dataset = dict(
        type=process_hf_dataset,
        dataset=dict(type=load_dataset, path='json', data_files=dict(train=data_path)),
        tokenizer=tokenizer,
        max_length=max_length,
        dataset_map_fn=None,
        template_map_fn=dict(
            type=template_map_fn_factory, template=prompt_template),
        remove_unused_columns=True,
        shuffle_before_pack=True,
        pack_to_max_length=pack_to_max_length)

    train_dataloader = dict(
        batch_size=batch_size,
        num_workers=dataloader_num_workers,
        dataset=train_dataset,
        sampler=dict(type=DefaultSampler, shuffle=True),
        collate_fn=dict(type=default_collate_fn))

    #######################################################################
    #                    PART 4  Scheduler & Optimizer                    #
    #######################################################################
    # optimizer
    optim_wrapper = dict(
        type=AmpOptimWrapper,
        optimizer=dict(
            type=optim_type, lr=lr, betas=betas, weight_decay=weight_decay),
        clip_grad=dict(max_norm=max_norm, error_if_nonfinite=False),
        accumulative_counts=accumulative_counts,
        loss_scale='dynamic',
        dtype='float16')

    # learning policy
    # More information: https://github.com/open-mmlab/mmengine/blob/main/docs/en/tutorials/param_scheduler.md  # noqa: E501
    param_scheduler = dict(
        type=CosineAnnealingLR,
        eta_min=0.0,
        by_epoch=True,
        T_max=max_epochs,
        convert_to_iter_based=True)

    # train, val, test setting
    train_cfg = dict(by_epoch=True, max_epochs=max_epochs, val_interval=1)

    #######################################################################
    #                           PART 5  Runtime                           #
    #######################################################################
    # Log the dialogue periodically during the training process, optional
    custom_hooks = [
        dict(type=DatasetInfoHook, tokenizer=tokenizer),
        dict(
            type=EvaluateChatHook,
            tokenizer=tokenizer,
            every_n_iters=evaluation_freq,
            evaluation_inputs=evaluation_inputs,
            system=SYSTEM,
            prompt_template=prompt_template)
    ]

    # configure default hooks
    default_hooks = dict(
        # record the time of every iteration.
        timer=dict(type=IterTimerHook),
        # print log every 100 iterations.
        logger=dict(type=LoggerHook, interval=10),
        # enable the parameter scheduler.
        param_scheduler=dict(type=ParamSchedulerHook),
        # save checkpoint per epoch.
        checkpoint=dict(type=CheckpointHook, interval=1),
        # set sampler seed in distributed evrionment.
        sampler_seed=dict(type=DistSamplerSeedHook),
    )

    # configure environment
    env_cfg = dict(
        # whether to enable cudnn benchmark
        cudnn_benchmark=False,
        # set multi process parameters
        mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
        # set distributed parameters
        dist_cfg=dict(backend='nccl'),
    )

    # set visualizer
    visualizer = None

    # set log level
    log_level = 'INFO'

    # load from which checkpoint
    load_from = None

    # whether to resume training from the loaded checkpoint
    resume = False

    # Defaults to use random seed and disable `deterministic`
    randomness = dict(seed=None, deterministic=False)

![Alt text](Pic/Bg-Pic-6.png)

### **è¿›è¡Œå¾®è°ƒ**

![Alt text](Pic/Bg-Pic-7.png)

### **å¾®è°ƒåå‚æ•°è½¬æ¢/åˆå¹¶**

**è®­ç»ƒåçš„pthæ ¼å¼å‚æ•°è½¬Hugging Faceæ ¼å¼**

    # åˆ›å»ºç”¨äºå­˜æ”¾Hugging Faceæ ¼å¼å‚æ•°çš„hfæ–‡ä»¶å¤¹
    mkdir /root/personal_assistant/config/work_dirs/hf

    export MKL_SERVICE_FORCE_INTEL=1

    # é…ç½®æ–‡ä»¶å­˜æ”¾çš„ä½ç½®
    export CONFIG_NAME_OR_PATH=/root/personal_assistant/config/internlm_chat_7b_qlora_oasst1_e3_copy.py

    # æ¨¡å‹è®­ç»ƒåå¾—åˆ°çš„pthæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
    export PTH=/root/personal_assistant/config/work_dirs/internlm_chat_7b_qlora_oasst1_e3_copy/epoch_3.pth

    # pthæ–‡ä»¶è½¬æ¢ä¸ºHugging Faceæ ¼å¼åå‚æ•°å­˜æ”¾çš„ä½ç½®
    export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf

    # æ‰§è¡Œå‚æ•°è½¬æ¢
    xtuner convert pth_to_hf $CONFIG_NAME_OR_PATH $PTH $SAVE_PATH

**Mergeæ¨¡å‹å‚æ•°**

    export MKL_SERVICE_FORCE_INTEL=1
    export MKL_THREADING_LAYER='GNU'

    # åŸå§‹æ¨¡å‹å‚æ•°å­˜æ”¾çš„ä½ç½®
    export NAME_OR_PATH_TO_LLM=/root/personal_assistant/model/Shanghai_AI_Laboratory/internlm-chat-7b

    # Hugging Faceæ ¼å¼å‚æ•°å­˜æ”¾çš„ä½ç½®
    export NAME_OR_PATH_TO_ADAPTER=/root/personal_assistant/config/work_dirs/hf

    # æœ€ç»ˆMergeåçš„å‚æ•°å­˜æ”¾çš„ä½ç½®
    mkdir /root/personal_assistant/config/work_dirs/hf_merge
    export SAVE_PATH=/root/personal_assistant/config/work_dirs/hf_merge

    # æ‰§è¡Œå‚æ•°Merge
    xtuner convert merge \
        $NAME_OR_PATH_TO_LLM \
        $NAME_OR_PATH_TO_ADAPTER \
        $SAVE_PATH \
        --max-shard-size 2GB

### **ç½‘é¡µDEMO**

**å®‰è£…ç½‘é¡µDemoæ‰€éœ€ä¾èµ–**

    pip install streamlit==1.24.0

**ä¸‹è½½InternLMé¡¹ç›®ä»£ç **

    # åˆ›å»ºcodeæ–‡ä»¶å¤¹ç”¨äºå­˜æ”¾InternLMé¡¹ç›®ä»£ç 
    mkdir /root/personal_assistant/code && cd /root/personal_assistant/code
    git clone https://github.com/InternLM/InternLM.git



**è¿è¡Œæ•ˆæœå¦‚ä¸Š(åŸºç¡€ä½œä¸š)**

## **é‡è¦ Bug ç¬”è®°**

## é“¾æ¥
**è§†é¢‘ï¼šXTuner å¤§æ¨¡å‹å•å¡ä½æˆæœ¬å¾®è°ƒå®æˆ˜**

https://www.bilibili.com/video/BV1yK4y1B75J/?spm_id_from=333.788&vd_source=cb911a92ddd7e0d930b1daa60c3fc181