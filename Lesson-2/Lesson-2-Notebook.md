# ğŸ˜**ä¹¦ç”ŸÂ·æµ¦è¯­(InternLM)-openLesson-2**ğŸ˜
> **â€œå€˜è‹¥ä½ æƒ³å¾æœä¸–ç•Œï¼Œä½ å°±å¾—å¾æœä½ è‡ªå·±ã€‚â€ -> é™€æ€å¦¥è€¶å¤«æ–¯åŸº**
## **ä½¿ç”¨ä¹¦ç”ŸÂ·æµ¦è¯­å®ŒæˆLLMçš„(â€œHello Worldâ€)**
![Alt text](Pic/Bg-Pic-1.png)
å¤§æ¨¡å‹çš„å®šä¹‰æ˜¯éå¸¸ç®€å•çš„ï¼Œå³â€œäººå·¥æ™ºèƒ½é¢†åŸŸä¸­å‚æ•°æ•°é‡å·¨å¤§ã€æ‹¥æœ‰åºå¤§è®¡ç®—èƒ½åŠ›å’Œå‚æ•°è§„æ¨¡çš„æ¨¡å‹ã€‚â€å…¶ç‰¹ç‚¹åŠåº”ç”¨åˆ†ä¸ºä¸‰ä¸ªä¸»ä½“ï¼š

1. åˆ©ç”¨å¤§é‡æ•°æ®è¿›è¡Œè®­ç»ƒ
2. æ‹¥æœ‰æ•°åäº¿ç”šè‡³æ•°åƒäº¿ä¸ªå‚æ•°
3. æ¨¡å‹åœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºæƒŠäººçš„æ€§èƒ½ <- **ä¸ªäººè®¤ä¸ºè¿™å¥è¯å®é™…ä¸Šä¸å¤ªåˆé€‚,å¤šæ•°æƒ…å†µä¸‹å¤§è¯­è¨€æ¨¡å‹èƒ½è¡¨ç°å‡ºå¼ºå¤§çš„æ³›åŒ–èƒ½åŠ›ï¼Œä½†ä¸ä¸€å®šæ˜¯é’ˆå¯¹æŸä¸ªç‰¹å®šä»»åŠ¡çš„ä¸“ä¸šèƒ½åŠ›ï¼Œè¿™æ˜¯ä¸ä¸“ç”¨æ¨¡å‹ç›¸åŒºåˆ«çš„**

![Alt text](Pic/Bg-Pic-2.png)

## **InternLM-Chat-7B æ™ºèƒ½å¯¹è¯ Demo**
![Alt text](Pic/Bg-Pic-3.png)

åŸºç¡€ç›®æ ‡ï¼š
+ ä½¿ç”¨ InternLM-Chat-7B æ¨¡å‹ç”Ÿæˆ 300 å­—çš„å°æ•…äº‹
+ ç†Ÿæ‚‰ hugging face ä¸‹è½½åŠŸèƒ½ï¼Œä½¿ç”¨ huggingface_hub python åŒ…ï¼Œä¸‹è½½ InternLM-20B çš„ config.json æ–‡ä»¶åˆ°æœ¬åœ°

### InternLM-Chat-7B æ¨¡å‹ && å°æ•…äº‹
#### (1) ç¯å¢ƒå‡†å¤‡
**åœ¨ InternStudio å¹³å°ä¸­é€‰æ‹© A100(1/4) çš„é…ç½®ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºé•œåƒé€‰æ‹© Cuda11.7-condaï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š**

![Alt text](Pic/Bg-Pic-4.png)

**è¿›å…¥ conda ç¯å¢ƒä¹‹åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä»æœ¬åœ°å…‹éš†ä¸€ä¸ªå·²æœ‰çš„ pytorch 2.0.1 çš„ç¯å¢ƒ**

    bash # è¯·æ¯æ¬¡ä½¿ç”¨ jupyter lab æ‰“å¼€ç»ˆç«¯æ—¶åŠ¡å¿…å…ˆæ‰§è¡Œ bash å‘½ä»¤è¿›å…¥ bash ä¸­
    conda create --name internlm-demo --clone=/root/share/conda_envs/internlm-base

**ç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¿€æ´»ç¯å¢ƒ**

    conda activate internlm-demo

**å¹¶åœ¨ç¯å¢ƒä¸­å®‰è£…è¿è¡Œ demo æ‰€éœ€è¦çš„ä¾èµ–ã€‚**

    python -m pip install --upgrade pip

    pip install modelscope==1.9.5
    pip install transformers==4.35.2
    pip install streamlit==1.24.0
    pip install sentencepiece==0.1.99
    pip install accelerate==0.24.1

#### (2) æ¨¡å‹ä¸‹è½½

**InternStudio å¹³å°çš„ share ç›®å½•ä¸‹å·²ç»ä¸ºæˆ‘ä»¬å‡†å¤‡äº†å…¨ç³»åˆ—çš„ InternLM æ¨¡å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥ç›´æ¥å¤åˆ¶å³å¯ã€‚ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤å¤åˆ¶ï¼š**

    mkdir -p /root/model/Shanghai_AI_Laboratory
    cp -r /root/share/temp/model_repos/internlm-chat-7b /root/model/Shanghai_AI_Laboratory

**ä¹Ÿå¯ä»¥ä½¿ç”¨æ¯”è¾ƒé€šç”¨çš„æ–¹æ³•(æ¨è)ï¼šåœ¨ /root è·¯å¾„ä¸‹æ–°å»ºç›®å½• modelï¼Œåœ¨ç›®å½•ä¸‹æ–°å»º download.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œç²˜è´´ä»£ç åè®°å¾—ä¿å­˜æ–‡ä»¶ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚å¹¶è¿è¡Œ python /root/model/download.py æ‰§è¡Œä¸‹è½½ï¼Œæ¨¡å‹å¤§å°ä¸º 14 GBï¼Œä¸‹è½½æ¨¡å‹å¤§æ¦‚éœ€è¦ 10~20 åˆ†é’Ÿï¼Œå…·ä½“ä»£ç å¦‚ä¸‹ï¼š**

    import torch
    from modelscope import snapshot_download, AutoModel, AutoTokenizer
    import os
    model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-chat-7b', cache_dir='/root/model', revision='v1.0.3')

#### (3) ä»£ç å‡†å¤‡

**é¦–å…ˆ clone ä»£ç ï¼Œåœ¨ /root è·¯å¾„ä¸‹æ–°å»º code ç›®å½•ï¼Œç„¶ååˆ‡æ¢è·¯å¾„, clone ä»£ç ã€‚**

    cd /root/code
    git clone https://gitee.com/internlm/InternLM.git

**åˆ‡æ¢ commit ç‰ˆæœ¬ï¼Œä¸æ•™ç¨‹ commit ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼Œå¯ä»¥è®©å¤§å®¶æ›´å¥½çš„å¤ç°ã€‚**

    cd InternLM
    git checkout 3028f07cb79e5b1d7342f4ad8d11efad3fd13d17

#### (4) Terminal Demo

    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM


    model_name_or_path = "/root/model/Shanghai_AI_Laboratory/internlm-chat-7b"

    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, trust_remote_code=True, torch_dtype=torch.bfloat16, device_map='auto')
    model = model.eval()

    system_prompt = """You are an AI assistant whose name is InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­).
    - InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) is a conversational language model that is developed by Shanghai AI Laboratory (ä¸Šæµ·äººå·¥æ™ºèƒ½å®éªŒå®¤). It is designed to be helpful, honest, and harmless.
    - InternLM (ä¹¦ç”ŸÂ·æµ¦è¯­) can understand and communicate fluently in the language chosen by the user such as English and ä¸­æ–‡.
    """

    messages = [(system_prompt, '')]

    print("=============Welcome to InternLM chatbot, type 'exit' to exit.=============")

    while True:
        input_text = input("User  >>> ")
        input_text = input_text.replace(' ', '')
        if input_text == "exit":
            break
        response, history = model.chat(tokenizer, input_text, history=messages)
        messages.append((input_text, response))
        print(f"robot >>> {response}")

#### (5) Web Demo

**è¿è¡Œ /root/code/InternLM ç›®å½•ä¸‹çš„ web_demo.py æ–‡ä»¶ï¼Œè¾“å…¥ä»¥ä¸‹å‘½ä»¤åï¼Œé…ç½®æœ¬åœ°ç«¯å£ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨æœ¬åœ°æµè§ˆå™¨è¾“å…¥ http://127.0.0.1:6006 å³å¯ã€‚**

    bash
    conda activate internlm-demo  
    # é¦–æ¬¡è¿›å…¥ vscode ä¼šé»˜è®¤æ˜¯ base ç¯å¢ƒï¼Œæ‰€ä»¥é¦–å…ˆåˆ‡æ¢ç¯å¢ƒ
    cd /root/code/InternLM
    streamlit run web_demo.py --server.address 127.0.0.1 --server.port 6006

#### (6) ç›®æ ‡æ•ˆæœå‘ˆç° - æ•…äº‹ç¼–å†™
![Alt text](Pic/Bg-Pic-5.png)
### å…³äº hugging face çš„ç›¸å…³ä½¿ç”¨æ–¹æ³•
#### (1) Hugging Face 

**ä½¿ç”¨ Hugging Face å®˜æ–¹æä¾›çš„ huggingface-cli å‘½ä»¤è¡Œå·¥å…·ã€‚å®‰è£…ä¾èµ–:**

    pip install -U huggingface_hub

**ç„¶åæ–°å»º python æ–‡ä»¶ï¼Œå¡«å…¥ä»¥ä¸‹ä»£ç ï¼Œè¿è¡Œå³å¯ã€‚**

---
* resume-downloadï¼šæ–­ç‚¹ç»­ä¸‹
* local-dirï¼šæœ¬åœ°å­˜å‚¨è·¯å¾„ã€‚ï¼ˆlinux ç¯å¢ƒä¸‹éœ€è¦å¡«å†™ç»å¯¹è·¯å¾„ï¼‰
---

    import os
    # ä¸‹è½½æ¨¡å‹
    os.system('huggingface-cli download --resume-download internlm/internlm-chat-7b --local-dir your_path')

**ä»¥ä¸‹å†…å®¹å°†å±•ç¤ºä½¿ç”¨ huggingface_hub ä¸‹è½½æ¨¡å‹ä¸­çš„éƒ¨åˆ†æ–‡ä»¶**

    import os 
    from huggingface_hub import hf_hub_download  # Load model directly 

    hf_hub_download(repo_id="internlm/internlm-7b", filename="config.json")

#### (2) ModelScope

**ä½¿ç”¨ modelscope ä¸­çš„ snapshot_download å‡½æ•°ä¸‹è½½æ¨¡å‹ï¼Œç¬¬ä¸€ä¸ªå‚æ•°ä¸ºæ¨¡å‹åç§°ï¼Œå‚æ•° cache_dir ä¸ºæ¨¡å‹çš„ä¸‹è½½è·¯å¾„ã€‚(æ³¨æ„ï¼šcache_dir æœ€å¥½ä¸ºç»å¯¹è·¯å¾„)**

**å®‰è£…ä¾èµ–ï¼š**

    pip install modelscope==1.9.5
    pip install transformers==4.35.2

**åœ¨å½“å‰ç›®å½•ä¸‹æ–°å»º python æ–‡ä»¶ï¼Œå¡«å…¥ä»¥ä¸‹ä»£ç ï¼Œè¿è¡Œå³å¯ã€‚**

    import torch
    from modelscope import snapshot_download, AutoModel, AutoTokenizer
    import os
    model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-chat-7b', cache_dir='your path', revision='master')

#### (3) ç›®æ ‡æ•ˆæœå‘ˆç° - ä¸‹è½½ç›¸å…³æ¨¡å‹
![Alt text](Pic/Bg-Pic-6.png)
---
![Alt text](Pic/Bg-Pic-7.png)

>ğŸ˜€**è‡³æ­¤ï¼ŒåŸºç¡€ä»»åŠ¡å·²ç»å®Œæˆ**ğŸ˜€

## **Lagent æ™ºèƒ½ä½“å·¥å…·è°ƒç”¨ Demo**
![Alt text](Pic/Bg-Pic-8.png)
### Lagent å®‰è£…
**é¦–å…ˆåˆ‡æ¢è·¯å¾„åˆ° /root/code å…‹éš† lagent ä»“åº“ï¼Œå¹¶é€šè¿‡ "pip install -e ." æºç å®‰è£… Lagent**

    cd /root/code
    git clone https://gitee.com/internlm/lagent.git
    cd /root/code/lagent
    git checkout 511b03889010c4811b1701abb153e02b8e94fb5e 
    # å°½é‡ä¿è¯å’Œæ•™ç¨‹commitç‰ˆæœ¬ä¸€è‡´
    pip install -e . # æºç å®‰è£…

![Alt text](Pic/Bg-Pic-9.png)

**æˆ‘ä»¬å¯ä»¥å‚è€ƒåŒæ ·çš„æ–¹æ³•ï¼Œåˆ‡æ¢åˆ° VScode é¡µé¢ï¼Œè¿è¡ŒæˆåŠŸåï¼Œé…ç½®æœ¬åœ°ç«¯å£ï¼Œå°†ç«¯å£æ˜ å°„åˆ°æœ¬åœ°ã€‚åœ¨æœ¬åœ°æµè§ˆå™¨è¾“å…¥ http://127.0.0.1:6006 å³å¯ã€‚

    streamlit run /root/code/lagent/examples/react_web_demo.py --server.address 127.0.0.1 --server.port 6006

### Demo è¿è¡Œæ•ˆæœ
![Alt text](Pic/Bg-Pic-10.png)

## **æµ¦è¯­Â·çµç¬”å›¾æ–‡ç†è§£åˆ›ä½œ Demo**

### ç¯å¢ƒå‡†å¤‡

**è¿›å…¥ conda ç¯å¢ƒä¹‹åï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤ä»æœ¬åœ°å…‹éš†ä¸€ä¸ªå·²æœ‰çš„ pytorch 2.0.1 çš„ç¯å¢ƒ**

    /root/share/install_conda_env_internlm_base.sh xcomposer-demo

![Alt text](Pic/Bg-Pic-11.png)

**ç„¶åä½¿ç”¨ä»¥ä¸‹å‘½ä»¤æ¿€æ´»ç¯å¢ƒ**

    conda activate xcomposer-demo

**æ¥ä¸‹æ¥è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼Œå®‰è£… transformersã€gradio ç­‰ä¾èµ–åŒ…ã€‚**

    pip install transformers==4.33.1 timm==0.4.12 sentencepiece==0.1.99 gradio==3.44.4 markdown2==2.4.10 xlsxwriter==3.1.2 einops accelerate

### æ¨¡å‹ä¸‹è½½

**å®‰è£… modelscope**

    pip install modelscope==1.9.5

![Alt text](Pic/Bg-Pic-12.png)

**åœ¨ /root/model è·¯å¾„ä¸‹æ–°å»º download.py æ–‡ä»¶å¹¶åœ¨å…¶ä¸­è¾“å…¥ä»¥ä¸‹å†…å®¹ï¼Œå¹¶è¿è¡Œ python /root/model/download.py æ‰§è¡Œä¸‹è½½**

    import torch
    from modelscope import snapshot_download, AutoModel, AutoTokenizer
    import os
    model_dir = snapshot_download('Shanghai_AI_Laboratory/internlm-xcomposer-7b', cache_dir='/root/model', revision='master')

### ä»£ç å‡†å¤‡

**åœ¨ /root/code git clone InternLM-XComposer ä»“åº“çš„ä»£ç **

    cd /root/code
    git clone https://gitee.com/internlm/InternLM-XComposer.git
    cd /root/code/InternLM-XComposer
    git checkout 3e8c79051a1356b9c388a6447867355c0634932d  
    # æœ€å¥½ä¿è¯å’Œæ•™ç¨‹çš„ commit ç‰ˆæœ¬ä¸€è‡´

### Demo è¿è¡Œæ•ˆæœ
![Alt text](Pic/Bg-Pic-13.png)
+ **åªä½¿ç”¨æ¨ç†ï¼Œ(1/2) A100 éƒ½å¾ˆå‹‰å¼º**

## é“¾æ¥
**è§†é¢‘ï¼šè½»æ¾ç©è½¬ä¹¦ç”ŸÂ·æµ¦è¯­å¤§æ¨¡å‹è¶£å‘³Demo**

https://www.bilibili.com/video/BV1Ci4y1z72H/?spm_id_from=333.999.0.0&vd_source=cb911a92ddd7e0d930b1daa60c3fc181
